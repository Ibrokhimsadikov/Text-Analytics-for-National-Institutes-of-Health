---
title: "Text Analytics"
author: "Ibrokhim Sadikov"
date: "September 20, 2019"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(data.table)
library(dplyr)
library(tidytext)
library(ggplot2)
library(wordcloud)
library(reshape2)
library(tm)
library(topicmodels)
library(slam)

```



```{r cars}
data <- fread("psychcentral_data.csv", sep=",", header=T, strip.white = T, na.strings = c("NA","NaN","","?")) 
head(data, n=5)
```


##3.1.	(1 point) What are the column names in the data? 
##3.2.	(1 point) How many rows does this data have? 
```{r }
colnames(data)
nrow(data)
```

##4.	Using libraries dplyr and tidytext, we will  tokenize column q_content. 
```{r }
tidy_text<-data %>%
  unnest_tokens(word, q_content)
tidy_text[1:15]

```

##Now we  remove the stop-words.
```{r }
data(stop_words)

tidy_text <- tidy_text %>%
  anti_join(stop_words)
```

##Now we are  counting the number of tokens.
```{r }
tidy_text %>%
  count(word, sort = TRUE) 
```
##What are the top five tokens returned?

##-im
##-dont
##-feel
##-time
##-life
      
##Using library ggplot2 we will  create a visualization that shows the frequency of the tokens that appeared for at least 2000 times.  the visualization is below:                                      
```{r }


tidy_text %>%
  count(word, sort = TRUE) %>%
  filter(n > 2000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat='identity') +
  xlab(NULL) +
  coord_flip()
```

##Based on the results in 4.2., would you suggest stemming on this text? Why? Bring one example from the visualization above that shows stemming should be done on this text? 

***-friend vs friends: I would suggest stemming as example above shows both of these words has the same root and thus appearing them twice brings extra redundancy.***


##Installing SnowballC package using install.packages("SnowballC", repos = "https://cran.r-project.org"). Use library SnowballC to stem q_content using the code below

```{r }
library(SnowballC)
tidy_text <- data %>%
 	unnest_tokens(word, q_content) %>%
mutate(word = wordStem(word)) 

```

##Now we remove the stop-words.  
```{r }
 data(stop_words)

tidy_text <- tidy_text %>%
  anti_join(stop_words) 
  
```

```{r }
 tidy_text %>%
  count(word, sort = TRUE)

```

##what are the top five tokens after stemming?

##-wa
##-thi
##-im
##-feel
##-dont

##Using library ggplot2 we will  create a visualization that shows the frequency of the tokens that appeared for at least 4000 times. 

```{r }
 tidy_text %>%
  count(word, sort = TRUE) %>%
  filter(n > 4000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat='identity') +
  xlab(NULL) +
  coord_flip()

```


##Using  library wordcloud we will  create a word cloud with the 200 most used tokens. The visualization is below:

```{r }
tidy_text %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 200))

```

##Creating  a color-coded word cloud based on sentiment. Using the most frequent 100 tokens for positive and negative words. The word cloud is in the space below:

```{r }
 

tidy_text %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 100)

```

##Repeating  all the steps in question 4 but this time for column answers.

```{r }
 tidy_text2<-data %>%
  unnest_tokens(word, answers)
tidy_text2[1:15]
```


```{r }
 data(stop_words)

tidy_text2 <- tidy_text2 %>%
  anti_join(stop_words)
```


```{r }
 tidy_text2 %>%
  count(word, sort = TRUE)
```
##What are the top five tokens returned? 

##-dont
##-feel
##-people
##-youre
##-time


##Using library ggplot2 we will create a visualization that shows the frequency of the tokens that appeared for at least 4000 times. The visualization below:

```{r }
  tidy_text2 %>%
  count(word, sort = TRUE) %>%
  filter(n > 4000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat='identity') +
  xlab(NULL) +
  coord_flip()
```

##Stemming for 'answers' column
```{r }
 
tidy_text2 <- data %>%
 	unnest_tokens(word, answers) %>%
mutate(word = wordStem(word)) 

```

```{r }
 data(stop_words)

tidy_text2 <- tidy_text2 %>%
  anti_join(stop_words)
```

```{r }
 tidy_text2 %>%
  count(word, sort = TRUE)
```

##Now what are the top five tokens after stemming?

##-thi
##-ar
##-feel
##-dont
##-wa


##Using library ggplot2 we will create a visualization that shows the frequency of the tokens that appeared for at least 6000 times.   The visualization below:

```{r }
 tidy_text2 %>%
  count(word, sort = TRUE) %>%
  filter(n > 6000) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_bar(stat='identity') +
  xlab(NULL) +
  coord_flip()
```

##Using library wordcloud we will create a word cloud with the 200 most used tokens. Paste the visualization below:

```{r }
 tidy_text2 %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 200))
```

##Creating a color-coded word cloud based on sentiment. Using the most frequent 100 tokens for positive and negative words. The word cloud in the space below:

```{r }
 tidy_text2 %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#F8766D", "#00BFC4"),
                   max.words = 100)
```

#Topic MOdelling

###Performing topic-modeling on q_content

```{r }
 data <- data[1:1000,] # We perform LDA on the rows 1 through 1000 in the data.
corpus <- Corpus(VectorSource(data$q_content), readerControl=list(language="en"))
dtm <- DocumentTermMatrix(corpus, control = list(stopwords = TRUE, minWordLength = 2, removeNumbers = TRUE, removePunctuation = TRUE,  stemDocument = TRUE))
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ] #remove all docs without words
lda <- LDA(dtm.new, k = 5) # k is the number of topics to be found.

```

##The code above will create the beta scores for each document per topic (k = 5). Then create bar plots (similar to what we created in class) for each topic for 10 tokens (top_n(10, beta)). Paste the visualization below.


```{r }
library(dplyr)
lda_td <- tidy(lda)
lda_td
top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() 
```

##Based on the visualization in 6.1., what can you say about k? Would you try a larger k or a smaller k? 

***ANSWER: From the above plot we can see that in all the topics most common words are: dont , like , just ,know , time. We need more different topics for better observe  Corpus. That is why we increase K****

##6.3.1.	 K = 2. Paste your visualization in the space below:

```{r }
 lda <- LDA(dtm.new, k = 2)
lda_td <- tidy(lda)
lda_td
top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

##K = 3. Paste your visualization in the space below:

```{r }
 lda <- LDA(dtm.new, k = 3)
lda_td <- tidy(lda)
lda_td
top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```


##K = 4. Paste your visualization in the space below:
```{r }
 lda <- LDA(dtm.new, k = 4)
lda_td <- tidy(lda)
lda_td
top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

##K = 10. Paste your visualization in the space below:

```{r }
 lda <- LDA(dtm.new, k = 10)
lda_td <- tidy(lda)
lda_td
top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

##Based on the results recommend the number of topics that would be appropriate for this corpus.

**I would definetely choose K=10, as smaller ks' are not producing worthwhile topics while k=10 introducing some new topics like mom, dad, relationship, depression.**

##7.	Using the following code to perform topic-modeling on ANSWER
```{r }
 data <- data[1:1000,] # We perform LDA on the rows 1 through 1000 in the data.
corpus <- Corpus(VectorSource(data$answers), readerControl=list(language="en"))
dtm <- DocumentTermMatrix(corpus, control = list(stopwords = TRUE, minWordLength = 2, removeNumbers = TRUE, removePunctuation = TRUE,  stemDocument = TRUE))
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ] #remove all docs without words
lda <- LDA(dtm.new, k = 10) # k is the number of topics to be found.

```

##7.1.	(5 points) The code BELOW will create the beta scores for each document per topic (k = 10). Then create bar plots (similar to what we created in class) for each topic for 10 tokens (top_n(10, beta)). Paste the visualization below.

```{r }
library(dplyr)
lda_td <- tidy(lda)
lda_td
top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
 
```

##7.2.	(5 points) Based on the visualization in 7.1., are the tokens in all topics similar? Then what can you say about k? Would you try a larger k or a smaller k?

*** It is difficult to decide from the first k. That is why we need to try different both smaller or larger ks' to see and indetify clear differences. Above plot shows us topics almost with similar variations with introducing few topics***

##7.3.	(10 points) Repeat 7.1. with the following ks:
##7.3.1.	 K = 2. Paste your visualization in the space below:

```{r }
 lda <- LDA(dtm.new, k = 2)
lda_td <- tidy(lda)
lda_td
top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

##7.3.2.	 K = 8. Paste your visualization in the space below:

```{r }
 lda <- LDA(dtm.new, k = 8)
lda_td <- tidy(lda)
lda_td
top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

##7.3.3.	 K = 11. Paste your visualization in the space below:

```{r }
 lda <- LDA(dtm.new, k = 11)
lda_td <- tidy(lda)
lda_td
top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

##K = 14. Paste your visualization in the space below:

```{r }
 lda <- LDA(dtm.new, k = 14)
lda_td <- tidy(lda)
lda_td
top_terms <- lda_td %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

##7.3.5Based on the results recommend the number of topics that would be appropriate for this corpus. 

***ANSWER:Again I would go with largest K=14, as our data is pretty small tuning by each K is giving similar results with some minor increase in the topic. More new topics were introduced in K=14.***



##8.	(20 points) Suppose that you are a researcher who works for National Institutes of Health (NIH). You are working on a project that aims to identify the most important reasons for mental disorders. Based on your analysis above, can we propose any hypothesis about the reasons for mental disorders in the society? Please explain. 

 ***ANSWER: Analyzing and we could somehow say after hypertuning of our sentiment and topic modelling, we can draw some useful conclusion. From the visualization of word cloud with  we can observe words like   love,   trust   ,support, luck,   smile,   smart,   sweet,   are  positive sentiment  and words such as depress ,hate, bad ,hard stress , broke ,pain ,mad,  represents negative sentiment. Even with topic modeling the both the  columns the   tops   words   in  the  topics  include   relationship,   good, love , mom, people, school, friends etc***
 
 *** So most of the mental disorders are related to the relationship as we saw many attributes like friends, mom, dad, people. Their stress or depression can also be main reasons for that. Generally, the society they are surrounded with and how they are treated by this comminity also reflected on our analysis.The main reason to mental disorder can also be human to human charectarisctics ***
 
 ***Therefore, maintaning good relationship, in other words the healthy one, with family, friends, and parents can bring positive impact on positive feelings like love, trust, and feel of support***

