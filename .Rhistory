mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_bar(stat='identity') +
xlab(NULL) +
coord_flip()
library(SnowballC)
tidy_text <- data %>%
unnest_tokens(word, q_content) %>%
mutate(word = wordStem(word))
library(SnowballC)
tidy_text <- data %>%
unnest_tokens(word, q_content) %>%
mutate(word = wordStem(word))
data(stop_words)
tidy_text <- tidy_text %>%
anti_join(stop_words)
tidy_text %>%
count(word, sort = TRUE)
tidy_text %>%
count(word, sort = TRUE) %>%
filter(n > 4000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_bar(stat='identity') +
xlab(NULL) +
coord_flip()
install.packages("wordcloud")
library(tidyverse)
library(data.table)
library(dplyr)
library(tidytext)
library(ggplot2)
library(wordcloud)
tidy_text %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 200))
tidy_text %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 200))
tidy_text %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 200))
install.packages("reshape2")
library(tidyverse)
library(data.table)
library(dplyr)
library(tidytext)
library(ggplot2)
library(wordcloud)
library(reshape2)
tidy_text %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
tidy_text2<-data %>%
unnest_tokens(word, answers)
tidy_text2[1:15]
data(stop_words)
tidy_text2 <- tidy_text2 %>%
anti_join(stop_words)
tidy_text2 %>%
count(word, sort = TRUE)
tidy_text2 %>%
count(word, sort = TRUE) %>%
filter(n > 4000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_bar(stat='identity') +
xlab(NULL) +
coord_flip()
tidy_text2 <- data %>%
unnest_tokens(word, answers) %>%
mutate(word = wordStem(word))
data(stop_words)
tidy_text2 <- tidy_text2 %>%
anti_join(stop_words)
tidy_text2 %>%
count(word, sort = TRUE)
tidy_text2 %>%
count(word, sort = TRUE) %>%
filter(n > 4000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_bar(stat='identity') +
xlab(NULL) +
coord_flip()
tidy_text2 %>%
count(word, sort = TRUE) %>%
filter(n > 6000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_bar(stat='identity') +
xlab(NULL) +
coord_flip()
tidy_text2 %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 200))
tidy_text2 %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 200))
tidy_text2 %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 200))
tidy_text2 %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
library(tidyverse)
library(data.table)
library(dplyr)
library(tidytext)
library(ggplot2)
library(wordcloud)
library(reshape2)
library(RTextTools)
install.packages("tm")
install.packages("topicmodels")
install.packages("slam")
install.packages("RTextTools")
library(tidyverse)
library(data.table)
library(dplyr)
library(tidytext)
library(ggplot2)
library(wordcloud)
library(reshape2)
library(tm)
library(topicmodels)
library(slam)
data <- data[1:1000,] # We perform LDA on the rows 1 through 1000 in the data.
corpus <- Corpus(VectorSource(data$q_content), readerControl=list(language="en"))
dtm <- DocumentTermMatrix(corpus, control = list(stopwords = TRUE, minWordLength = 2, removeNumbers = TRUE, removePunctuation = TRUE,  stemDocument = TRUE))
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ] #remove all docs without words
lda <- LDA(dtm.new, k = 5) # k is the number of topics to be found.
top_terms <- lda %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms <- lda %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms <- lda %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
install.packages("dplyr")
library(dplyr)
top_terms <- lda %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
library(dplyr)
top_terms <- lda %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
library(dplyr)
lda_td <- tidy(lda)
lda_td
top_terms <- lda_td %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_bar(stat = "identity", show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
library(dplyr)
lda_td <- tidy(lda)
lda_td
top_terms <- lda_td %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_bar(stat = "identity", show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
library(dplyr)
lda_td <- tidy(lda)
lda_td
top_terms <- lda_td %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_bar(stat = "identity", show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
lda <- LDA(dtm.new, k = 2)
lda_td <- tidy(lda)
lda_td
top_terms <- lda_td %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_bar(stat = "identity", show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
lda <- LDA(dtm.new, k = 3)
lda_td <- tidy(lda)
lda_td
top_terms <- lda_td %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_bar(stat = "identity", show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
lda <- LDA(dtm.new, k = 4)
lda_td <- tidy(lda)
lda_td
top_terms <- lda_td %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_bar(stat = "identity", show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
lda <- LDA(dtm.new, k = 10)
lda_td <- tidy(lda)
lda_td
top_terms <- lda_td %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_bar(stat = "identity", show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
library(tidyverse)
library(data.table)
library(dplyr)
library(tidytext)
library(ggplot2)
library(wordcloud)
library(reshape2)
library(tm)
library(topicmodels)
library(slam)
library(tidyverse)
library(data.table)
library(dplyr)
library(tidytext)
library(ggplot2)
library(wordcloud)
library(reshape2)
library(tm)
library(topicmodels)
library(slam)
data <- fread("psychcentral_data.csv", sep=",", header=T, strip.white = T, na.strings = c("NA","NaN","","?"))
head(data, n=5)
colnames(data)
nrow(data)
tidy_text<-data %>%
unnest_tokens(word, q_content)
tidy_text[1:15]
data(stop_words)
tidy_text <- tidy_text %>%
anti_join(stop_words)
tidy_text %>%
count(word, sort = TRUE)
tidy_text %>%
count(word, sort = TRUE) %>%
filter(n > 2000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_bar(stat='identity') +
xlab(NULL) +
coord_flip()
library(SnowballC)
tidy_text <- data %>%
unnest_tokens(word, q_content) %>%
mutate(word = wordStem(word))
data(stop_words)
tidy_text <- tidy_text %>%
anti_join(stop_words)
tidy_text %>%
count(word, sort = TRUE)
tidy_text %>%
count(word, sort = TRUE) %>%
filter(n > 4000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_bar(stat='identity') +
xlab(NULL) +
coord_flip()
tidy_text %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 200))
tidy_text %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
tidy_text2<-data %>%
unnest_tokens(word, answers)
tidy_text2[1:15]
tidy_text2 %>%
count(word, sort = TRUE)
tidy_text2<-data %>%
unnest_tokens(word, answers)
tidy_text2[1:15]
data(stop_words)
tidy_text2 <- tidy_text2 %>%
anti_join(stop_words)
tidy_text2 %>%
count(word, sort = TRUE)
tidy_text2 %>%
count(word, sort = TRUE) %>%
filter(n > 4000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_bar(stat='identity') +
xlab(NULL) +
coord_flip()
tidy_text2 <- data %>%
unnest_tokens(word, answers) %>%
mutate(word = wordStem(word))
tidy_text2 <- data %>%
unnest_tokens(word, answers) %>%
mutate(word = wordStem(word))
library(tidyverse)
library(data.table)
library(dplyr)
library(tidytext)
library(ggplot2)
library(wordcloud)
library(reshape2)
library(tm)
library(topicmodels)
library(slam)
data <- fread("psychcentral_data.csv", sep=",", header=T, strip.white = T, na.strings = c("NA","NaN","","?"))
head(data, n=5)
colnames(data)
nrow(data)
tidy_text<-data %>%
unnest_tokens(word, q_content)
tidy_text[1:15]
data(stop_words)
tidy_text <- tidy_text %>%
anti_join(stop_words)
tidy_text %>%
count(word, sort = TRUE)
tidy_text %>%
count(word, sort = TRUE) %>%
filter(n > 2000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_bar(stat='identity') +
xlab(NULL) +
coord_flip()
library(SnowballC)
tidy_text <- data %>%
unnest_tokens(word, q_content) %>%
mutate(word = wordStem(word))
data(stop_words)
tidy_text <- tidy_text %>%
anti_join(stop_words)
tidy_text %>%
count(word, sort = TRUE)
tidy_text %>%
count(word, sort = TRUE) %>%
filter(n > 4000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_bar(stat='identity') +
xlab(NULL) +
coord_flip()
tidy_text %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 200))
tidy_text %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
tidy_text2<-data %>%
unnest_tokens(word, answers)
tidy_text2[1:15]
data(stop_words)
tidy_text2 <- tidy_text2 %>%
anti_join(stop_words)
tidy_text2 %>%
count(word, sort = TRUE)
tidy_text2 %>%
count(word, sort = TRUE) %>%
filter(n > 4000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_bar(stat='identity') +
xlab(NULL) +
coord_flip()
tidy_text2 <- data %>%
unnest_tokens(word, answers) %>%
mutate(word = wordStem(word))
data(stop_words)
tidy_text2 <- tidy_text2 %>%
anti_join(stop_words)
tidy_text2 %>%
count(word, sort = TRUE)
tidy_text2 %>%
count(word, sort = TRUE) %>%
filter(n > 6000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_bar(stat='identity') +
xlab(NULL) +
coord_flip()
tidy_text2 %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 200))
tidy_text2 %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("#F8766D", "#00BFC4"),
max.words = 100)
data <- data[1:1000,] # We perform LDA on the rows 1 through 1000 in the data.
corpus <- Corpus(VectorSource(data$q_content), readerControl=list(language="en"))
dtm <- DocumentTermMatrix(corpus, control = list(stopwords = TRUE, minWordLength = 2, removeNumbers = TRUE, removePunctuation = TRUE,  stemDocument = TRUE))
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ] #remove all docs without words
lda <- LDA(dtm.new, k = 5) # k is the number of topics to be found.
library(dplyr)
lda_td <- tidy(lda)
library(dplyr)
lda_td <- tidy(lda)
lda <- LDA(dtm.new, k = 2)
lda_td <- tidy(lda)
lda <- LDA(dtm.new, k = 2)
lda_td <- tidy(lda)
data <- data[1:1000,] # We perform LDA on the rows 1 through 1000 in the data.
corpus <- Corpus(VectorSource(data$q_content), readerControl=list(language="en"))
dtm <- DocumentTermMatrix(corpus, control = list(stopwords = TRUE, minWordLength = 2, removeNumbers = TRUE, removePunctuation = TRUE,  stemDocument = TRUE))
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ] #remove all docs without words
lda <- LDA(dtm.new, k = 5) # k is the number of topics to be found.
library(dplyr)
lda_td <- tidy(lda)
data <- data[1:1000,] # We perform LDA on the rows 1 through 1000 in the data.
corpus <- Corpus(VectorSource(data$q_content), readerControl=list(language="en"))
dtm <- DocumentTermMatrix(corpus, control = list(stopwords = TRUE, minWordLength = 2, removeNumbers = TRUE, removePunctuation = TRUE,  stemDocument = TRUE))
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ] #remove all docs without words
lda <- LDA(dtm.new, k = 5) # k is the number of topics to be found.
library(dplyr)
lda_td <- tidy(lda)
library(tidytext)
lda_td <- tidy(lda)
library(tidytext)
lda_td <- tidy(lda)
library(tidyverse)
library(data.table)
library(dplyr)
library(tidytext)
library(ggplot2)
library(wordcloud)
library(reshape2)
library(tm)
library(topicmodels)
library(slam)
library(tidytext)
lda_td <- tidy(lda)
library(tidytext)
lda_td <- tidy(lda)
